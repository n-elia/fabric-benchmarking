{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXP_3: Performances varying the number of organizations\n",
    "\n",
    "EXP_3 benchmarks are executed against the [HyperWatchdog](https://gitlab.com/ecs-lab/hyper-watchdog) smart contract, with:\n",
    "\n",
    "- the maximum possible number of transactions requested per second (obtained by leveraging the fixed-load mode of Caliper, that keeps sending transaction requests to always keep a backlog queue of pending transactions)\n",
    "- real-world network conditions\n",
    "- varying the number of organizations\n",
    "\n",
    "The aim is to evaluate the maximum performance that can be achieved by the system, and how it scales with the number of organizations.\n",
    "Note that in this experiment we are using an endorsement policy that requires all the organizations to endorse the transaction, so the number of organizations is also the number of required endorsers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reminder: restart kernel after installing packages!\n",
    "%pip install pandas\n",
    "%pip install plotly\n",
    "%pip install nbformat\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Settings\n",
    "NOTEBOOK_NAME = \"exp_3\"\n",
    "TARGET_CALIPER_REPORTS = \"exp_3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing\n",
    "\n",
    "The following code block parses the data from the html reports generated by Caliper and stores data into a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_caliper_reports_dir_path = [\"..\", \"launcher\", \"results\", \"exp_3_multiple_orgs\"] # Reports dir\n",
    "target_caliper_reports_dir = os.path.join(*target_caliper_reports_dir_path)\n",
    "target_reports_filename_begin_with = TARGET_CALIPER_REPORTS # Filename filter\n",
    "export_filename = NOTEBOOK_NAME + \"_results.csv\"\n",
    "\n",
    "columns = [\n",
    "    \"Bechmark\",\n",
    "    \"Name\",\n",
    "    \"Succ\",\n",
    "    \"Fail\",\n",
    "    \"Send Rate (TPS)\",\n",
    "    \"Max Latency (s)\",\n",
    "    \"Min Latency (s)\",\n",
    "    \"Avg Latency (s)\",\n",
    "    \"Throughput (TPS)\",\n",
    "]\n",
    "rows = []\n",
    "\n",
    "for filename in os.listdir(target_caliper_reports_dir):\n",
    "    # Filter by filename\n",
    "    filepath = os.path.join(target_caliper_reports_dir, filename)\n",
    "    if target_reports_filename_begin_with not in filepath:\n",
    "        continue\n",
    "\n",
    "    # Skip directories\n",
    "    if os.path.isdir(filepath):\n",
    "        continue\n",
    "\n",
    "    with open(filepath, \"r\") as fp:\n",
    "        state = 0\n",
    "        for line in fp.readlines():\n",
    "            line = line.strip()\n",
    "            if state == 0 and \"Performance metrics\" in line:\n",
    "                state = 1\n",
    "            if state > 0 and \"</table>\" in line:\n",
    "                state = 0\n",
    "            # if state==1 and \"<th>\" in line:\n",
    "            #    key = [l.split(\"</th>\")[0] for l in line.split(\"<th>\")[1:]]\n",
    "            #    print(key)\n",
    "            if state == 1 and \"<td>\" in line:\n",
    "                rows.append(\n",
    "                    [filepath] + [l.split(\"</td>\")[0] for l in line.split(\"<td>\")[1:]]\n",
    "                )\n",
    "                # print(values)\n",
    "            if state > 0:\n",
    "                pass\n",
    "\n",
    "with open(export_filename, \"w\") as fp:\n",
    "    print(\";\".join(columns), file=fp)\n",
    "    for row in rows:\n",
    "        print(\";\".join(row), file=fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and preprocess\n",
    "\n",
    "The following code block imports the .csv file generated by the parsing code block.\n",
    "Data is stored in a pandas dataframe.\n",
    "\n",
    "Also, new columns are added to the dataframe to store the data in a more convenient way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(export_filename, sep=\";\")\n",
    "\n",
    "# Create two new columns \"Chunk Length\" and \"Requested TPS\" from the column \"Name\"\n",
    "df[\"Chunk Length\"] = df[\"Name\"].apply(lambda x: [int(i) for i in \"\".join([i for i in x if not i.isalpha()]).strip().split()][0])\n",
    "# df[\"Requested TPS\"] = df[\"Name\"].apply(lambda x: [int(i) for i in \"\".join([i for i in x if not i.isalpha()]).strip().split()][1])\n",
    "\n",
    "# Create a new column \"Benchmark ID\" from the column \"Bechmark\"\n",
    "df[\"Benchmark ID\"] = df[\"Bechmark\"].apply(lambda x: x.split(\"/\")[len(target_caliper_reports_dir_path)].split(\"_\")[1].split(\".\")[0])\n",
    "print(\"Benchmark IDs detected:\", df[\"Benchmark ID\"].unique())\n",
    "\n",
    "# Create a new column \"Success Rate\" from the column \"Succ\" and \"Fail\"\n",
    "df[\"Success Rate\"] = df[\"Succ\"] / (df[\"Succ\"] + df[\"Fail\"])\n",
    "\n",
    "# Create a new column \"Transaction Size (KiB)\" from the column \"Chunk Length\" considering that each transaction is 580 bytes\n",
    "# Round the result as integer\n",
    "df[\"Transaction Size (Byte)\"] = df[\"Chunk Length\"] * 580\n",
    "df[\"Transaction Size (Byte)\"] = df[\"Transaction Size (Byte)\"].apply(lambda x: round(x))\n",
    "\n",
    "# Function to extract new features from the filename\n",
    "def extract_feature_from_filename(inputFilename, featureTag: str, targetType = None):\n",
    "    filename = inputFilename.split(\"/\")[-1]\n",
    "    params = filename.split(\".\")[0].split(\"_\")\n",
    "    params[-1] = params[-1].split(\".\")[0]\n",
    "\n",
    "    if featureTag in params:\n",
    "        i = params.index(featureTag)\n",
    "        # Cast to the target type\n",
    "        if targetType is not None:\n",
    "            if targetType == \"int\":\n",
    "                return int(params[i + 1])\n",
    "            elif targetType == \"float\":\n",
    "                return float(params[i + 1])\n",
    "            elif targetType == \"str\":\n",
    "                return params[i + 1]\n",
    "        return params[i + 1]\n",
    "    else:\n",
    "        # If the feature is not found, return \"ideal\"\n",
    "        return \"ideal\"\n",
    "\n",
    "# Create new columns from the filename\n",
    "for feature_tag, column_name, column_type in [\n",
    "    (\"pl\", \"Network Packet Loss (%)\", \"int\"),\n",
    "    (\"thr\", \"Network Max Throughput (Mbps)\", \"int\"),\n",
    "    (\"del\", \"Network Avg Latency (ms)\", \"int\"),\n",
    "    (\"jit\", \"Network Jitter (ms)\", \"int\"),\n",
    "    (\"orgs\", \"Number of Organizations\", \"int\"),\n",
    "    ]:\n",
    "    df[column_name] = df[\"Bechmark\"].apply(lambda x: extract_feature_from_filename(\n",
    "        inputFilename=x,\n",
    "        featureTag=feature_tag,\n",
    "        targetType=column_type,\n",
    "        ))\n",
    "\n",
    "# Evaluate data consistency\n",
    "def eval_consistency(df: pd.DataFrame):\n",
    "    # Create new column \"Consistency\" of type boolean and set it to True\n",
    "    df[\"Consistency\"] = True\n",
    "\n",
    "    for row in df.iterrows():\n",
    "        row_consistency = True\n",
    "\n",
    "        filename = row[1][\"Bechmark\"].split(\"/\")[-1]\n",
    "        params = filename.split(\".\")[0].split(\"_\")\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            if params[i] == 'tps':\n",
    "                if int(params[i+1]) != row[1][\"Requested TPS\"]:\n",
    "                    row_consistency = False\n",
    "            elif params[i] == 'chunklen':\n",
    "                if int(params[i+1]) != row[1][\"Chunk Length\"]:\n",
    "                    row_consistency = False\n",
    "        \n",
    "        if row_consistency == False:\n",
    "            df.loc[row[0], \"Consistency\"] = False\n",
    "\n",
    "eval_consistency(df)\n",
    "\n",
    "# Check that none of the rows is inconsistent\n",
    "if False in df[\"Consistency\"].values:\n",
    "    print(\"Inconsistency detected!\")\n",
    "\n",
    "    print(\"Inconsistent rows:\")\n",
    "    print(df[df[\"Consistency\"] == False][\"Bechmark\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis\n",
    "\n",
    "### Average throughput versus number of organizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def group_network_conditions(s):\n",
    "    network_conditions = \"\"\n",
    "\n",
    "    for column_name in [\"Network Packet Loss (%)\", \"Network Max Throughput (Mbps)\", \"Network Avg Latency (ms)\", \"Network Jitter (ms)\"]:\n",
    "        if s[column_name] != \"ideal\":\n",
    "            name = column_name.lstrip(\"Network\").split(\"(\")[0].strip() # Before the first \"(\"\n",
    "            value = s[column_name] # The value\n",
    "            unit = str(column_name.split('(')[1].strip(\")\")).strip() # Inside the parentheses\n",
    "            network_conditions += f\",<br>{name} {value}{unit}\"\n",
    "    \n",
    "    if network_conditions == \"\":\n",
    "        return \"Ideal\"\n",
    "    else:\n",
    "        return network_conditions[5:]\n",
    "\n",
    "# Create a new column \"Conditions\" grouping the network conditions\n",
    "df[\"Network Conditions\"] = df.apply(lambda x: group_network_conditions(x), axis=1)\n",
    "\n",
    "cds = df[\"Network Conditions\"].unique() # All the different network conditions\n",
    "for i in range(len(cds)):\n",
    "    print(f\"Network condition {i}: {cds[i]}\")\n",
    "\n",
    "# Sort dataframe\n",
    "df_sorted = df.sort_values(by=[\"Number of Organizations\", \"Transaction Size (Byte)\"])\n",
    "\n",
    "# Plot\n",
    "fig = px.line(df_sorted, x='Transaction Size (Byte)', color=\"Number of Organizations\", y=\"Throughput (TPS)\", markers=True, template=\"simple_white\")\n",
    "\n",
    "# Set log scale\n",
    "# fig.update_layout(\n",
    "#     xaxis_type=\"log\",\n",
    "#     # yaxis_type=\"log\",\n",
    "# )\n",
    "# fig.update_xaxes(range=[4.96, 6.04]) # Logarithmic range\n",
    "\n",
    "# Add title\n",
    "fig.update_layout(\n",
    "    title=\"Throughput vs Transaction Size for different Number of Organizations\",\n",
    "    title_x=0.5, # Center title\n",
    ")\n",
    "\n",
    "# Add grid\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        showgrid=True,\n",
    "        gridwidth=1,\n",
    "        gridcolor='LightGrey',\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        showgrid=True,\n",
    "        gridwidth=1,\n",
    "        gridcolor='LightGrey',\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Set border pane lines\n",
    "fig.update_xaxes(showline=True, linewidth=1, linecolor='black', mirror=True)\n",
    "fig.update_yaxes(showline=True, linewidth=1, linecolor='black', mirror=True)\n",
    "\n",
    "# Set cathegories title\n",
    "fig.update_legends(title_text=\"Number of Organizations\")\n",
    "\n",
    "# Center the legend\n",
    "fig.update_layout(legend=dict(\n",
    "    yanchor=\"middle\",\n",
    "    y=0.5,\n",
    "    xanchor=\"left\",\n",
    "    x=1.02\n",
    "))\n",
    "\n",
    "# Save EPS figure\n",
    "# fig.write_image(NOTEBOOK_NAME + \"throughput_vs_nodes.eps\", width=800, height=450, scale=1)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average latency versus number of organizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def group_network_conditions(s):\n",
    "    network_conditions = \"\"\n",
    "\n",
    "    for column_name in [\"Network Packet Loss (%)\", \"Network Max Throughput (Mbps)\", \"Network Avg Latency (ms)\", \"Network Jitter (ms)\"]:\n",
    "        if s[column_name] != \"ideal\":\n",
    "            name = column_name.lstrip(\"Network\").split(\"(\")[0].strip() # Before the first \"(\"\n",
    "            value = s[column_name] # The value\n",
    "            unit = str(column_name.split('(')[1].strip(\")\")).strip() # Inside the parentheses\n",
    "            network_conditions += f\",<br>{name} {value}{unit}\"\n",
    "    \n",
    "    if network_conditions == \"\":\n",
    "        return \"Ideal\"\n",
    "    else:\n",
    "        return network_conditions[5:]\n",
    "\n",
    "# Create a new column \"Conditions\" grouping the network conditions\n",
    "df[\"Network Conditions\"] = df.apply(lambda x: group_network_conditions(x), axis=1)\n",
    "\n",
    "cds = df[\"Network Conditions\"].unique() # All the different network conditions\n",
    "for i in range(len(cds)):\n",
    "    print(f\"Network condition {i}: {cds[i]}\")\n",
    "\n",
    "# Sort dataframe\n",
    "df_sorted = df.sort_values(by=[\"Number of Organizations\", \"Transaction Size (Byte)\"])\n",
    "\n",
    "# Plot\n",
    "fig = px.line(df_sorted, x='Transaction Size (Byte)', color=\"Number of Organizations\", y=\"Avg Latency (s)\", markers=True, template=\"simple_white\")\n",
    "\n",
    "# Set log scale\n",
    "# fig.update_layout(\n",
    "#     xaxis_type=\"log\",\n",
    "#     # yaxis_type=\"log\",\n",
    "# )\n",
    "# fig.update_xaxes(range=[4.96, 6.04]) # Logarithmic range\n",
    "\n",
    "# Add title\n",
    "fig.update_layout(\n",
    "    title=\"Avg Latency vs Transaction Size for different Number of Organizations\",\n",
    "    title_x=0.5, # Center title\n",
    ")\n",
    "\n",
    "# Add grid\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        showgrid=True,\n",
    "        gridwidth=1,\n",
    "        gridcolor='LightGrey',\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        showgrid=True,\n",
    "        gridwidth=1,\n",
    "        gridcolor='LightGrey',\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Set border pane lines\n",
    "fig.update_xaxes(showline=True, linewidth=1, linecolor='black', mirror=True)\n",
    "fig.update_yaxes(showline=True, linewidth=1, linecolor='black', mirror=True)\n",
    "\n",
    "# Set cathegories title\n",
    "fig.update_legends(title_text=\"Number of Organizations\")\n",
    "\n",
    "# Center the legend\n",
    "fig.update_layout(legend=dict(\n",
    "    yanchor=\"middle\",\n",
    "    y=0.5,\n",
    "    xanchor=\"left\",\n",
    "    x=1.02\n",
    "))\n",
    "\n",
    "# Save EPS figure\n",
    "# fig.write_image(NOTEBOOK_NAME + \"throughput_vs_nodes.eps\", width=800, height=450, scale=1)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Success rate versus number of organizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def group_network_conditions(s):\n",
    "    network_conditions = \"\"\n",
    "\n",
    "    for column_name in [\"Network Packet Loss (%)\", \"Network Max Throughput (Mbps)\", \"Network Avg Latency (ms)\", \"Network Jitter (ms)\"]:\n",
    "        if s[column_name] != \"ideal\":\n",
    "            name = column_name.lstrip(\"Network\").split(\"(\")[0].strip() # Before the first \"(\"\n",
    "            value = s[column_name] # The value\n",
    "            unit = str(column_name.split('(')[1].strip(\")\")).strip() # Inside the parentheses\n",
    "            network_conditions += f\",<br>{name} {value}{unit}\"\n",
    "    \n",
    "    if network_conditions == \"\":\n",
    "        return \"Ideal\"\n",
    "    else:\n",
    "        return network_conditions[5:]\n",
    "\n",
    "# Create a new column \"Conditions\" grouping the network conditions\n",
    "df[\"Network Conditions\"] = df.apply(lambda x: group_network_conditions(x), axis=1)\n",
    "\n",
    "cds = df[\"Network Conditions\"].unique() # All the different network conditions\n",
    "for i in range(len(cds)):\n",
    "    print(f\"Network condition {i}: {cds[i]}\")\n",
    "\n",
    "# Sort dataframe\n",
    "df_sorted = df.sort_values(by=[\"Number of Organizations\", \"Transaction Size (Byte)\"])\n",
    "\n",
    "# Plot\n",
    "fig = px.line(df_sorted, x='Transaction Size (Byte)', color=\"Number of Organizations\", y=\"Success Rate\", markers=True, template=\"simple_white\")\n",
    "\n",
    "# Set log scale\n",
    "# fig.update_layout(\n",
    "#     xaxis_type=\"log\",\n",
    "#     # yaxis_type=\"log\",\n",
    "# )\n",
    "# fig.update_xaxes(range=[4.96, 6.04]) # Logarithmic range\n",
    "\n",
    "# Add title\n",
    "fig.update_layout(\n",
    "    title=\"Success Rate vs Transaction Size for different Number of Organizations\",\n",
    "    title_x=0.5, # Center title\n",
    ")\n",
    "\n",
    "# Add grid\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        showgrid=True,\n",
    "        gridwidth=1,\n",
    "        gridcolor='LightGrey',\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        showgrid=True,\n",
    "        gridwidth=1,\n",
    "        gridcolor='LightGrey',\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Set border pane lines\n",
    "fig.update_xaxes(showline=True, linewidth=1, linecolor='black', mirror=True)\n",
    "fig.update_yaxes(showline=True, linewidth=1, linecolor='black', mirror=True)\n",
    "\n",
    "# Set cathegories title\n",
    "fig.update_legends(title_text=\"Number of Organizations\")\n",
    "\n",
    "# Center the legend\n",
    "fig.update_layout(legend=dict(\n",
    "    yanchor=\"middle\",\n",
    "    y=0.5,\n",
    "    xanchor=\"left\",\n",
    "    x=1.02\n",
    "))\n",
    "\n",
    "# Save EPS figure\n",
    "# fig.write_image(NOTEBOOK_NAME + \"throughput_vs_nodes.eps\", width=800, height=450, scale=1)\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
